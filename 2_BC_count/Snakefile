### input, output and shell paths are all relative to the project directory ###

configfile: "config.yml"

from itertools import product
import glob
import re

#Restrict wildcards, so it doesnt create weird combinations
wildcard_constraints:
  Design='|'.join(config["Design"]),
  ScreenID='|'.join(config["ScreenID"]),
  FastqDir='|'.join(config["FastqDir"]),

# Load Input and Output directory from the config file
OutDir=config["OutDir"] 
InDir=config["InDir"]
AssDir=config["AssDir"]
Design = config["Design"]
ScreenID = config["ScreenID"]
FastqDir = config["FastqDir"]
LibraryName = config["LibraryName"]

# this checks whether a particular combination is among the ones that should be processed
# https://stackoverflow.com/questions/41185567/how-to-use-expand-in-snakemake-when-some-particular-combinations-of-wildcards-ar

def filter_combinator(combinator, combs):
    def filtered_combinator(*args, **kwargs):
        for wc_comb in combinator(*args, **kwargs):
            if frozenset(wc_comb) in combs:
                yield wc_comb
    return filtered_combinator

#Which deisgns go together
right_combinations = {
    frozenset({("Design", "LibASeqDesign"), ("FastqDir", "LibraryA_minPInitial"), ("LibraryName", "LibAminPInitial")}),
    frozenset({("Design", "LibASeqDesign"), ("FastqDir", "LibraryA_minPOptimized"), ("LibraryName", "LibAminPOpt")}),
    frozenset({("Design", "LibHSeqDesign"), ("FastqDir", "LibraryH_minP"), ("LibraryName", "LibHminP")}),
    frozenset({("Design", "LibHSeqDesign"), ("FastqDir", "LibraryH_minCMV"), ("LibraryName", "LibHminCMV")}),
    frozenset({("Design", "LibBSeqDesign"), ("FastqDir", "LibraryB_minP"), ("LibraryName", "LibBminP")})}

filtered_product = filter_combinator(product, right_combinations)   

# create a dictionary with every combination of samples for each screen
# store all samples
comb_list = []
ids = []
cellstate = []

pattern = r'(.*?)[DR]NA'

for si in ScreenID:
    for fd in FastqDir: 
        samples, = glob_wildcards(InDir+"/"+si+"_"+fd+"/{NucSample}_merged_R1.fastq.gz")
        for bc in samples:
            ids.append(bc)
            extracted_state = re.match(pattern, bc).group(1) if re.match(pattern, bc) else bc
            cellstate.append(extracted_state)
            comb_list.append({"ScreenID": si, "FastqDir": fd, "NucSample": bc, "cellstate": extracted_state})

comb_list = { frozenset(x.items()) for x in comb_list }
combinations = filter_combinator(product, comb_list)

#Rule all
rule all:
    input:
        expand(AssDir+"/{FastqDir}/{Design}_{LibraryName}_mapped_filtered.csv.gz", 
            filtered_product,
            FastqDir = config["FastqDir"],
            Design = config["Design"],
            LibraryName = config["LibraryName"]),
        expand(OutDir+"/{ScreenID}_{FastqDir}/filtered/State_{cellstate}/{NucSample}.map.csv.gz",
            combinations,
            ScreenID = config["ScreenID"],
            FastqDir = config["FastqDir"],
            cellstate = cellstate,
            NucSample = ids)


#Count BC for every sample, indepently from each other
rule count_BC_filtered:
    input:
        umi = InDir+"/{ScreenID}_{FastqDir}/{NucSample}_merged_R2.fastq.gz",
        fwd = InDir+"/{ScreenID}_{FastqDir}/{NucSample}_merged_R1.fastq.gz",
        rev = InDir+"/{ScreenID}_{FastqDir}/{NucSample}_merged_R3.fastq.gz",
        mapped = lambda wildcards: glob.glob(AssDir+"/{FastqDir}/*_mapped_filtered.csv.gz".format(FastqDir=wildcards.FastqDir))
    output:
        out = OutDir+"/{ScreenID}_{FastqDir}/filtered/State_{cellstate}/{NucSample}.map.csv.gz",
        stat = OutDir+"/{ScreenID}_{FastqDir}/filtered/State_{cellstate}/{NucSample}.umistat.txt",
        nonmapped = OutDir+"/{ScreenID}_{FastqDir}/filtered/State_{cellstate}/{NucSample}.nonmapped.csv.gz",
    conda:
       "MPRA_processing.yml"  
    log:
        OutDir+"/log_files/count_BC/{ScreenID}_{FastqDir}/filtered/{NucSample}_{cellstate}_countBC.log"
    params:
        MINRUMI = config["BCcounting"]["MINRUMI"],
        MAXRUMI = config["BCcounting"]["MAXRUMI"],
        downs = config["BCcounting"]["downs"],
        downsto = config["BCcounting"]["downsto"],   
    shell:
        "perl scripts/count_BC.pl {input.umi} {input.fwd} {input.rev} {input.mapped} {output.out} {params.MINRUMI} {params.MAXRUMI} {params.downs} {params.downsto} {output.stat} {output.nonmapped} " 
        "2> {log}"
